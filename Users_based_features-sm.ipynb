{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 12})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_users_data = pd.read_csv(\"data/WSB_data_with_emb.csv\",sep=';', usecols = ['created_utc','author','title','selftext'], low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_users_data= pd.read_csv(\"stocks/data/stocks_submissions_all.csv\",sep=';', usecols = ['created_utc','author','title','selftext'], low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_users_data= pd.read_csv(\"investing/data/investing_submissions_all.csv\",sep=';', usecols = ['created_utc','author','title','selftext'], low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 861752 entries, 0 to 861751\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   created_utc  861752 non-null  object\n",
      " 1   author       861752 non-null  object\n",
      " 2   selftext     223819 non-null  object\n",
      " 3   title        861752 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "wsb_users_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154506 entries, 0 to 154505\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   created_utc  154506 non-null  object\n",
      " 1   author       154506 non-null  object\n",
      " 2   selftext     132641 non-null  object\n",
      " 3   title        154506 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "stocks_users_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127769 entries, 0 to 127768\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   created_utc  127769 non-null  object\n",
      " 1   author       127769 non-null  object\n",
      " 2   selftext     122158 non-null  object\n",
      " 3   title        127767 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "investing_users_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_users_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-26 13:19:32</td>\n",
       "      <td>NovelAnteater</td>\n",
       "      <td></td>\n",
       "      <td>SNAP seems like a great long-term buy at this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_utc         author selftext  \\\n",
       "0  2021-11-26 13:19:32  NovelAnteater            \n",
       "\n",
       "                                               title  \n",
       "0  SNAP seems like a great long-term buy at this ...  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_users_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_users_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_users_data['selftext'] =investing_users_data['selftext'].apply(lambda x: x if not x=='[removed]' else \"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_users_data['selftext'] =stocks_users_data['selftext'].apply(lambda x: x if not x=='[removed]' else \"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_users_data['selftext'] =wsb_users_data['selftext'].apply(lambda x: x if not x=='[removed]' else \"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features : Part of Speech, Bag of Words, Sentiment, Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")#en_core_web_trf, _md\n",
    "\n",
    "#nlp = spacy.blank(\"en\")\n",
    "if \"spacytextblob\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"spacytextblob\")\n",
    "    nlp.rename_pipe(\"spacytextblob\", \"sentiment\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': 2, 'AUX': 1, 'SPACE': 1, 'VERB': 3, 'ADP': 2, 'ADJ': 2, 'NOUN': 1, 'SYM': 1, 'NUM': 2}\n",
      "{'looking': 1, 'buying': 1, 'bought': 1}\n",
      "{'awesome': 1, 'great': 1}\n",
      "{'Apple': 1, 'U.K.': 1}\n",
      "{'startup': 1}\n",
      "{'at': 1, 'for': 1}\n",
      "{'$': 1}\n",
      "{'1': 1, 'billion': 1}\n",
      "{'is': 1}\n",
      "{'Apple': 1, 'be': 1, ' ': 1, 'look': 1, 'at': 1, 'buy': 2, 'awesome': 1, 'U.K.': 1, 'great': 1, 'startup': 1, 'for': 1, '$': 1, '1': 1, 'billion': 1}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is  looking at buying awesome U.K. great startup for $1 billion bought\")\n",
    "\n",
    "print(dict(Counter([token.pos_ for token in doc])))\n",
    "#print(dict(Counter([token.tag_ for token in doc])))\n",
    "#print(dict(Counter([ent.text for ent in doc.ents if ent.label_ =='ORG'])))   \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='VERB'])))  \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='ADJ'])))  \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='PROPN'])))  \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='NOUN']))) \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='ADP']))) \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='SYM']))) \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='NUM']))) \n",
    "print(dict(Counter([token.text for token in doc if token.pos_ =='AUX']))) \n",
    "print (dict(Counter([token.lemma_ for token in doc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_stocks = pd.read_csv(\"data/nasdaq.csv\", sep=\",\")\n",
    "nasdaq_stocks_symbols = nasdaq_stocks[\"Symbol\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find stocks in a text given stock list\n",
    "import re\n",
    "regex = re.compile(\"[^a-zA-Z ]\")\n",
    "ticker_set = set(nasdaq_stocks_symbols)\n",
    "#list(filter(lambda word: word[0]=='s', text.split()))\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords= all_stopwords + ['good','big','nice','go','em','live','hear','plan', 'else',\n",
    "                                'it','best','get','new','see','next',\n",
    "                               'cash','well','post','two','real','play','hope','low','free','huge','open','pay','move','ever',\n",
    "                                'love','tell','life','gain','tech','fund','bit','ago','fact','net','talk','turn','app','main',\n",
    "                               'grow','kind','pump','beat','vs','true','ride','care','mind','man','link','via','job','safe',\n",
    "                               'plus','wish','team','save','user','base','onto','view','eat','flow','jan','fun','info','car',\n",
    "                               'peak','step','mark','five','cars','max','wave','cool','auto','six','loan','act','eye','roll','site',\n",
    "                                'cost','form','dd','run','four','fast','gold','key','drug','bill','hi','game','age','hes',\n",
    "                                'ryan','earn','uk','apps','sum','pays','rent','jobs','usa','self','coin','law','wash','blue'\n",
    "                               ]\n",
    "def calculate_mentioned_stocks(title, body=\"\"):\n",
    "    #content = regex.sub(\"\", str(title) + \" \" + str(body)).split(\" \")\n",
    "    text = str(title) + \" \" + str(body)   \n",
    "    tick_list = list(filter(lambda word: word[0]=='$', text.upper().split())) \n",
    "    #print(tick_list)\n",
    "    content = set(list([i[1:] for i in tick_list]))\n",
    "    #print(content)\n",
    "    tickers = list(ticker_set & content)\n",
    "    #print(tickers)\n",
    "    tickers2 = [word for word in text.split() if not word.lower() in all_stopwords]\n",
    "    #print(tickers2)\n",
    "    tickers2 = [x.upper() for x in tickers2  if len(x) > 1]\n",
    "    #print(tickers2)\n",
    "    #print()\n",
    "    return tickers +list(ticker_set & set(tickers2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'TSLA', 'MSFT', 'AAPL', 'TSLA', 'BB']\n"
     ]
    }
   ],
   "source": [
    "print(calculate_mentioned_stocks('TSLA  any aapl be $E  420.69 $TSLA is go GO is $msft else plan it get best hear new not big nice a good em CONFIRMED.Something,on live bb something'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tickers(title, body=\"\"):\n",
    "    tickers = dict(Counter(calculate_mentioned_stocks(title,body)))    \n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def text2bow(words: List[str], dictionary: Dict[str, int]):\n",
    "    word_frequences = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        word_frequences[dictionary[word]] += 1\n",
    " \n",
    "    #return list(word_frequences.items())\n",
    "    return word_frequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "#print('BOW representation:', text2bow(sample_text.split(), dictionary))\n",
    "#print('Dictionary:', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_features(text, body=\"\"):\n",
    "    if(len(str(text).lower()+\" \"+ str(body).lower()) < 3689602):\n",
    "       \n",
    "        doc = nlp(str(text).lower()+\" \"+ str(body).lower())\n",
    "        lemma_text = \" \".join([token.lemma_ for token in doc])\n",
    "        doc = nlp(lemma_text)\n",
    "        #part_of_speech():\n",
    "        ##count PROPN, VERB, ADP, NOUN, NUM\n",
    "        pos = dict(Counter([token.pos_ for token in doc]))\n",
    "        pos_tag = dict(Counter([token.tag_ for token in doc]))\n",
    "        \n",
    "        #Extract Nouns, Verbs and Adjectives\n",
    "        \n",
    "        nouns = dict(Counter([token.text for token in doc if token.pos_ =='NOUN']))\n",
    "        adjectives = dict(Counter([token.text for token in doc if token.pos_ =='ADJ']))\n",
    "        verbs =dict(Counter([token.text for token in doc if token.pos_ =='VERB']))\n",
    "        pronouns = dict(Counter([token.text for token in doc if token.pos_ =='PROPN']))      \n",
    "        symbols = dict(Counter([token.text for token in doc if token.pos_ =='SYM']))\n",
    "        numbers = dict(Counter([token.text for token in doc if token.pos_ =='NUM']))\n",
    "      \n",
    "        #named_entitles():\n",
    "        named_entities = dict(Counter([ent.label_ for ent in doc.ents]))\n",
    "        named_entities_org = dict(Counter([ent.text for ent in doc.ents if ent.label_ == 'ORG']))\n",
    "        #def bag_of_words():\n",
    "        token_list = [token.text for token in doc]\n",
    "        # Create list of word tokens after removing stopwords\n",
    "        filtered_sentence =[] \n",
    "\n",
    "        for word in token_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence.append(word) \n",
    "        bow=text2bow(filtered_sentence, dictionary)\n",
    "        #sentiment():\n",
    "        polarity = doc._.polarity        \n",
    "\n",
    "        return pd.Series([nouns,adjectives,verbs,pronouns,symbols,numbers,pos, pos_tag,named_entities,named_entities_org,polarity,bow,lemma_text])\n",
    "    else:\n",
    "         return pd.Series([0,0,0,0,0,0,0,0,0,0,0,0,0])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total posts by common users in each subreddit :common_elements [List 1,2,3]: 5637\n",
    "len(investing_users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154506"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks_users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wsb_users_data)  # 3X posts in WSB by same number of users in WSB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by author\n",
    "def create_dataset_groupby_author(df):\n",
    "    df_author_posts= df.groupby(['author']).agg({'created_utc': 'count'})\n",
    "    df['title'] = df['title'].astype(\"string\")\n",
    "    df.title = np.where(df.title.isnull(),\" \",df.title)\n",
    "    df_author_posts['title']= df.groupby(['author']).agg({'title': ' '.join})\n",
    "    df['selftext'] = df['selftext'].astype(\"string\")\n",
    "    df.selftext = np.where(df.selftext.isnull(),\" \",df.selftext)\n",
    "    df_author_posts['selftext'] = df.groupby(['author']).agg({'selftext': ' '.join})\n",
    "    #df['year'] = df['year'].astype(\"string\")\n",
    "    #df_author_posts['year']= df.groupby(['author']).agg({'year': ' '.join})\n",
    "    df_author_posts[[\"nouns\",\"adjectives\",\"verbs\",\"pronouns\",\"symbols\",'numbers',\"pos\",\"pos_tag\",\"NER\",\"org\",\"polarity\",\"bow\",\"lemma_text\"]]=df_author_posts.apply(lambda row: spacy_features(row['title'],row['selftext']),axis=1)\n",
    "    \n",
    "    df_author_posts['tickers'] = df_author_posts.apply(lambda row: find_tickers(row[\"title\"],row[\"selftext\"]), axis=1)\n",
    "    \n",
    "    return df_author_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary= {}\n",
    "nlp.max_length = 3689602  \n",
    "investing_users_groupdata = create_dataset_groupby_author(investing_users_data)\n",
    "dictionary_investing=dictionary\n",
    "investing_users_groupdata.to_csv('data/wsb_spacy/investing_users_groupdata_title_selftext.csv',sep=';',index=False)\n",
    "with open('data/wsb_spacy/dictionary_investing_group_title_selftext.json', 'w') as fp:\n",
    "        json.dump(dictionary_investing, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_users_groupdata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary= {}\n",
    "nlp.max_length = 3689602  \n",
    "stocks_users_groupdata = create_dataset_groupby_author(stocks_users_data)\n",
    "dictionary_stocks =dictionary\n",
    "stocks_users_groupdata.to_csv('data/wsb_spacy/stocks_users_groupdata_title_selftext.csv',sep=';',index=False)\n",
    "with open('data/wsb_spacy/dictionary_stocks_group_title_selftext.json', 'w') as fp:\n",
    "        json.dump(dictionary_stocks, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>symbols</th>\n",
       "      <th>numbers</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>NER</th>\n",
       "      <th>org</th>\n",
       "      <th>polarity</th>\n",
       "      <th>bow</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>tickers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>----The_Truth-----</th>\n",
       "      <td>1</td>\n",
       "      <td>How to play ETFs</td>\n",
       "      <td>I've been trading pennies and swinging &amp;lt;$50...</td>\n",
       "      <td>{'etfs': 2, 'trade': 1, 'penny': 1, 'swinge': ...</td>\n",
       "      <td>{'start': 1, 'different': 1, 'small': 1, 'ok':...</td>\n",
       "      <td>{'play': 1, 'be': 4, 'look': 1, 'invest': 1, '...</td>\n",
       "      <td>{'lt;$50': 1, 'amp;#x200b': 2}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'SCONJ': 3, 'PART': 7, 'VERB': 18, 'NOUN': 19...</td>\n",
       "      <td>{'WRB': 1, 'TO': 7, 'VB': 14, 'NN': 20, 'PRP':...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.078333</td>\n",
       "      <td>{0: 1, 1: 2, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: ...</td>\n",
       "      <td>how to play etfs I 've be trade penny and swin...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_utc             title  \\\n",
       "author                                              \n",
       "----The_Truth-----            1  How to play ETFs   \n",
       "\n",
       "                                                             selftext  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  I've been trading pennies and swinging &lt;$50...   \n",
       "\n",
       "                                                                nouns  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  {'etfs': 2, 'trade': 1, 'penny': 1, 'swinge': ...   \n",
       "\n",
       "                                                           adjectives  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  {'start': 1, 'different': 1, 'small': 1, 'ok':...   \n",
       "\n",
       "                                                                verbs  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  {'play': 1, 'be': 4, 'look': 1, 'invest': 1, '...   \n",
       "\n",
       "                                          pronouns symbols numbers  \\\n",
       "author                                                               \n",
       "----The_Truth-----  {'lt;$50': 1, 'amp;#x200b': 2}      {}      {}   \n",
       "\n",
       "                                                                  pos  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  {'SCONJ': 3, 'PART': 7, 'VERB': 18, 'NOUN': 19...   \n",
       "\n",
       "                                                              pos_tag NER org  \\\n",
       "author                                                                          \n",
       "----The_Truth-----  {'WRB': 1, 'TO': 7, 'VB': 14, 'NN': 20, 'PRP':...  {}  {}   \n",
       "\n",
       "                    polarity  \\\n",
       "author                         \n",
       "----The_Truth-----  0.078333   \n",
       "\n",
       "                                                                  bow  \\\n",
       "author                                                                  \n",
       "----The_Truth-----  {0: 1, 1: 2, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: ...   \n",
       "\n",
       "                                                           lemma_text tickers  \n",
       "author                                                                         \n",
       "----The_Truth-----  how to play etfs I 've be trade penny and swin...      {}  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_users_groupdata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary= {}\n",
    "nlp.max_length = 3689602  \n",
    "wsb_users_groupdata = create_dataset_groupby_author(wsb_users_data)\n",
    "dictionary_wsb =dictionary\n",
    "wsb_users_groupdata.to_csv('data/wsb_spacy/wsb_users_groupdata_title_selftext.csv',sep=';',index=False)\n",
    "with open('data/wsb_spacy/dictionary_wsb_group_title_selftext.json', 'w') as fp:\n",
    "        json.dump(dictionary_wsb, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_users_groupdata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features : Part of Speech, Bag of Words, Sentiment, Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb =pd.read_csv('data/wsb_spacy/wsb_users_groupdata_title_selftext.csv',sep=';')\n",
    "stocks =pd.read_csv('data/wsb_spacy/stocks_users_groupdata_title_selftext.csv',sep=';')\n",
    "investing =pd.read_csv('data/wsb_spacy/investing_users_groupdata_title_selftext.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(merged,mergedfrom):   \n",
    "    for k,v in mergedfrom.items():\n",
    "        if k in merged:\n",
    "            merged[k] += v\n",
    "        else:\n",
    "            merged[k] = v\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def merge_dict_column(dict_list,bow=False,total_posts=1):\n",
    "    #print(total_posts)\n",
    "    global_dict = {}\n",
    "    for dc in dict_list:\n",
    "        if bow:\n",
    "            # dc = eval(dc)\n",
    "            # if not isinstance(dc, int):\n",
    "             #   dc = dict(eval(re.sub(r\"<class '(\\w+)'>\", r'\\1', dc)))\n",
    "            #print(dict(eval(re.sub(r\"<class '(\\w+)'>\", r'\\1', dc))))\n",
    "            #return\n",
    "            try:\n",
    "                dc = dict(eval(re.sub(r\"<class '(\\w+)'>\", r'\\1', dc)))\n",
    "            except BaseException as err:\n",
    "                print(f\"Unexpected {err}, {type(err)}\")\n",
    "        else:\n",
    "            dc = literal_eval(dc)\n",
    "        if isinstance(dc, dict):    \n",
    "            global_dict = merge_dicts(global_dict,dc)\n",
    "    global_dict = {k: (v / total_posts)*100 for k, v in global_dict.items()}\n",
    "    return sorted(global_dict.items(), key=lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine all features as a one row i.e. add all tickers, Pos tags etc\n",
    "def combine_features(df):\n",
    "    posts_total = df['created_utc'].sum()\n",
    "    polarity = df['polarity'].sum() / posts_total   \n",
    "    bow= merge_dict_column(df['bow'].tolist(),True,posts_total)\n",
    "    nouns= merge_dict_column(df['nouns'].tolist(),False,posts_total)\n",
    "    #print(nouns)\n",
    "    #return\n",
    "    verbs= merge_dict_column(df['verbs'].tolist(),False,posts_total)\n",
    "    adjectives= merge_dict_column(df['adjectives'].tolist(),False,posts_total)\n",
    "    pronouns= merge_dict_column(df['pronouns'].tolist(),False,posts_total)\n",
    "    symbols= merge_dict_column(df['symbols'].tolist(),False,posts_total)\n",
    "    numbers= merge_dict_column(df['numbers'].tolist(),False,posts_total)\n",
    "    pos= merge_dict_column(df['pos'].tolist(),False,posts_total)\n",
    "    pos_tag= merge_dict_column(df['pos_tag'].tolist(),False,posts_total)\n",
    "    NER= merge_dict_column(df['NER'].tolist(),False,posts_total)\n",
    "    org= merge_dict_column(df['org'].tolist(),False,posts_total)\n",
    "    \n",
    "    tickers= merge_dict_column(df['tickers'].tolist(),False,posts_total)\n",
    "    \n",
    "    new_df = pd.DataFrame([[posts_total,polarity,nouns,adjectives,verbs,pronouns,symbols,numbers,pos,pos_tag,NER,org,bow,tickers]], columns = ['posts_total','polarity',\"nouns\",\"adjectives\",\"verbs\",\"pronouns\",\"symbols\",'numbers','pos','pos_tag','NER','org','bow','tickers'])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected 'int' object is not iterable, <class 'TypeError'>\n"
     ]
    }
   ],
   "source": [
    "wsb_combined = combine_features(wsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts_total</th>\n",
       "      <th>polarity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>symbols</th>\n",
       "      <th>numbers</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>NER</th>\n",
       "      <th>org</th>\n",
       "      <th>bow</th>\n",
       "      <th>tickers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861752</td>\n",
       "      <td>0.020628</td>\n",
       "      <td>[(stock, 19.092383887707832), (%, 15.347222866...</td>\n",
       "      <td>[(short, 11.379840139622537), (good, 7.5944123...</td>\n",
       "      <td>[(be, 40.34791912290311), (have, 20.6413214010...</td>\n",
       "      <td>[(🚀, 42.05537091877942), (gme, 6.2681606773178...</td>\n",
       "      <td>[($, 37.58239029326303), (/, 27.96233719213880...</td>\n",
       "      <td>[(one, 6.511618191776752), (1, 4.1990038897501...</td>\n",
       "      <td>[(NOUN, 982.245355972484), (PUNCT, 631.7127201...</td>\n",
       "      <td>[(NN, 971.0538530807007), (IN, 425.97487444183...</td>\n",
       "      <td>[(DATE, 51.70478281454525), (ORG, 49.881984608...</td>\n",
       "      <td>[(🚀, 32.63734810014946), (sec, 0.9589765965150...</td>\n",
       "      <td>[(3, 175.1732517011855), (51, 122.146162701101...</td>\n",
       "      <td>[(GME, 6.728153807591976), (AMC, 3.38682126644...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   posts_total  polarity                                              nouns  \\\n",
       "0       861752  0.020628  [(stock, 19.092383887707832), (%, 15.347222866...   \n",
       "\n",
       "                                          adjectives  \\\n",
       "0  [(short, 11.379840139622537), (good, 7.5944123...   \n",
       "\n",
       "                                               verbs  \\\n",
       "0  [(be, 40.34791912290311), (have, 20.6413214010...   \n",
       "\n",
       "                                            pronouns  \\\n",
       "0  [(🚀, 42.05537091877942), (gme, 6.2681606773178...   \n",
       "\n",
       "                                             symbols  \\\n",
       "0  [($, 37.58239029326303), (/, 27.96233719213880...   \n",
       "\n",
       "                                             numbers  \\\n",
       "0  [(one, 6.511618191776752), (1, 4.1990038897501...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [(NOUN, 982.245355972484), (PUNCT, 631.7127201...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(NN, 971.0538530807007), (IN, 425.97487444183...   \n",
       "\n",
       "                                                 NER  \\\n",
       "0  [(DATE, 51.70478281454525), (ORG, 49.881984608...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  [(🚀, 32.63734810014946), (sec, 0.9589765965150...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0  [(3, 175.1732517011855), (51, 122.146162701101...   \n",
       "\n",
       "                                             tickers  \n",
       "0  [(GME, 6.728153807591976), (AMC, 3.38682126644...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsb_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_all = combine_features(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts_total</th>\n",
       "      <th>polarity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>symbols</th>\n",
       "      <th>numbers</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>NER</th>\n",
       "      <th>org</th>\n",
       "      <th>bow</th>\n",
       "      <th>tickers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154506</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>[(stock, 73.42756915589038), (%, 36.6788344789...</td>\n",
       "      <td>[(good, 16.779283652414794), (new, 12.34256274...</td>\n",
       "      <td>[(be, 60.41642395764566), (have, 37.1545441600...</td>\n",
       "      <td>[(gt, 8.891564081653787), (amp;#x200b, 4.33640...</td>\n",
       "      <td>[($, 53.18110623535656), (/, 43.89926604792047...</td>\n",
       "      <td>[(one, 8.767944286953258), (1, 6.6016853714418...</td>\n",
       "      <td>[(NOUN, 1621.1972350588326), (PUNCT, 1017.4737...</td>\n",
       "      <td>[(NN, 1602.6225518750082), (IN, 725.3452940338...</td>\n",
       "      <td>[(DATE, 97.0085304130584), (CARDINAL, 63.41177...</td>\n",
       "      <td>[(fed, 1.3287509870166854), (🚀, 1.192833935251...</td>\n",
       "      <td>[(22, 244.58532354730562), (10, 209.8857002317...</td>\n",
       "      <td>[(GME, 1.3779400152744876), (AMC, 0.8498051855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   posts_total  polarity                                              nouns  \\\n",
       "0       154506  0.045298  [(stock, 73.42756915589038), (%, 36.6788344789...   \n",
       "\n",
       "                                          adjectives  \\\n",
       "0  [(good, 16.779283652414794), (new, 12.34256274...   \n",
       "\n",
       "                                               verbs  \\\n",
       "0  [(be, 60.41642395764566), (have, 37.1545441600...   \n",
       "\n",
       "                                            pronouns  \\\n",
       "0  [(gt, 8.891564081653787), (amp;#x200b, 4.33640...   \n",
       "\n",
       "                                             symbols  \\\n",
       "0  [($, 53.18110623535656), (/, 43.89926604792047...   \n",
       "\n",
       "                                             numbers  \\\n",
       "0  [(one, 8.767944286953258), (1, 6.6016853714418...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [(NOUN, 1621.1972350588326), (PUNCT, 1017.4737...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(NN, 1602.6225518750082), (IN, 725.3452940338...   \n",
       "\n",
       "                                                 NER  \\\n",
       "0  [(DATE, 97.0085304130584), (CARDINAL, 63.41177...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  [(fed, 1.3287509870166854), (🚀, 1.192833935251...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0  [(22, 244.58532354730562), (10, 209.8857002317...   \n",
       "\n",
       "                                             tickers  \n",
       "0  [(GME, 1.3779400152744876), (AMC, 0.8498051855...  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_all =combine_features(investing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts_total</th>\n",
       "      <th>polarity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>symbols</th>\n",
       "      <th>numbers</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>NER</th>\n",
       "      <th>org</th>\n",
       "      <th>bow</th>\n",
       "      <th>tickers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127769</td>\n",
       "      <td>0.053641</td>\n",
       "      <td>[(stock, 48.73639145645657), (%, 41.2619649523...</td>\n",
       "      <td>[(good, 19.172882311045715), (other, 16.007795...</td>\n",
       "      <td>[(be, 79.87774812356675), (have, 50.0058699684...</td>\n",
       "      <td>[(amp;#x200b, 7.497123715455236), (ira, 5.3933...</td>\n",
       "      <td>[($, 47.34481760051343), (/, 36.52216108758775...</td>\n",
       "      <td>[(one, 11.005016866376039), (10, 8.16943076959...</td>\n",
       "      <td>[(NOUN, 1913.6151961743458), (PUNCT, 1108.0708...</td>\n",
       "      <td>[(NN, 1910.2348770045942), (IN, 872.5168076763...</td>\n",
       "      <td>[(DATE, 96.18217251445968), (CARDINAL, 66.4762...</td>\n",
       "      <td>[(fed, 2.0043985630317214), (etfs, 1.769599824...</td>\n",
       "      <td>[(13, 292.86524900406204), (38, 248.4796781691...</td>\n",
       "      <td>[(EARN, 0.6918736156657718), (GME, 0.677003028...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   posts_total  polarity                                              nouns  \\\n",
       "0       127769  0.053641  [(stock, 48.73639145645657), (%, 41.2619649523...   \n",
       "\n",
       "                                          adjectives  \\\n",
       "0  [(good, 19.172882311045715), (other, 16.007795...   \n",
       "\n",
       "                                               verbs  \\\n",
       "0  [(be, 79.87774812356675), (have, 50.0058699684...   \n",
       "\n",
       "                                            pronouns  \\\n",
       "0  [(amp;#x200b, 7.497123715455236), (ira, 5.3933...   \n",
       "\n",
       "                                             symbols  \\\n",
       "0  [($, 47.34481760051343), (/, 36.52216108758775...   \n",
       "\n",
       "                                             numbers  \\\n",
       "0  [(one, 11.005016866376039), (10, 8.16943076959...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [(NOUN, 1913.6151961743458), (PUNCT, 1108.0708...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(NN, 1910.2348770045942), (IN, 872.5168076763...   \n",
       "\n",
       "                                                 NER  \\\n",
       "0  [(DATE, 96.18217251445968), (CARDINAL, 66.4762...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  [(fed, 2.0043985630317214), (etfs, 1.769599824...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0  [(13, 292.86524900406204), (38, 248.4796781691...   \n",
       "\n",
       "                                             tickers  \n",
       "0  [(EARN, 0.6918736156657718), (GME, 0.677003028...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_combined.to_csv('data/wsb_spacy/wsb_all_userBased_normalized.csv',sep=';')\n",
    "stocks_all.to_csv('data/wsb_spacy/stocks_all_userBased_normalized.csv',sep=';')\n",
    "investing_all.to_csv('data/wsb_spacy/investing_all_userBased_normalized.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_diff_operations(feature1_keys,feature2_keys,feature3_keys):\n",
    "    print(\"In WSB,Not in Stocks:\",list(set(feature1_keys) - set(feature2_keys)))\n",
    "    print(\"In WSB,Not in Investing:\",list(set(feature1_keys) - set(feature3_keys)))\n",
    "    print(\"Unique in WSB:\", (set(feature1_keys) - set(feature2_keys)) & (set(feature1_keys) - set(feature3_keys)))\n",
    "    print()\n",
    "    print(\"In Stocks,Not in WSB:\",list(set(feature2_keys) - set(feature1_keys)))\n",
    "    print(\"In Stocks,Not in Investing:\",list(set(feature2_keys) - set(feature3_keys)))\n",
    "    print(\"Unique in Stocks:\", (set(feature2_keys) - set(feature1_keys)) & (set(feature2_keys) - set(feature3_keys)))\n",
    "    print()\n",
    "    print(\"In Investing,Not in Stocks:\",list(set(feature3_keys) - set(feature2_keys)))\n",
    "    print(\"In Investing,Not in WSB:\",list(set(feature3_keys) - set(feature1_keys)))\n",
    "    print(\"Unique in Investing:\", (set(feature3_keys) - set(feature2_keys)) & (set(feature3_keys) - set(feature1_keys)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(feature_name,feature1,feature2,feature3=[],from_=0,till_=10): #input parameters: list of tuples\n",
    "    #seperate feature_keys and feature_counts\n",
    "    feature1_keys= list(list(zip(*feature1[from_:till_]))[0])\n",
    "    feature1_count= list(list(zip(*feature1[from_:till_]))[1])\n",
    "    feature2_keys= list(list(zip(*feature2[from_:till_]))[0])\n",
    "    feature2_count= list(list(zip(*feature2[from_:till_]))[1])\n",
    "    if feature3:\n",
    "        feature3_keys= list(list(zip(*feature3[from_:till_]))[0])\n",
    "        feature3_count= list(list(zip(*feature3[from_:till_]))[1])\n",
    "    #print(feature_keys1,feature1_count)\n",
    "    #intersect and find common_feature_keys\n",
    "    common_feature_keys = list(set(feature1_keys) & set(feature2_keys) & set(feature3_keys))\n",
    "    set_diff_operations(feature1_keys,feature2_keys,feature3_keys)\n",
    "    count=[]\n",
    "    feature2_count=[]\n",
    "    feature3_count=[]\n",
    "    for feature in common_feature_keys:\n",
    "        feature1_count=[]\n",
    "        feature1_count.append(feature)\n",
    "        feature1_count.append([count for word,count in feature1 if word == feature][0])\n",
    "        feature1_count.append([count for word,count in feature2 if word == feature][0])\n",
    "        feature1_count.append([count for word,count in feature3 if word == feature][0])\n",
    "        count.append(feature1_count)\n",
    "    plot_df = pd.DataFrame(count, columns =[feature_name, 'WSB', 'Stocks','Investing'], dtype = int)\n",
    "     #plot\n",
    "    plot_df.plot(figsize=(13,6),kind='bar',x=feature_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_(feature_name,feature1,feature2,feature3=[]): #input parameters: list of tuples\n",
    "    #seperate feature_keys and feature_counts\n",
    "    feature1_keys= list(list(zip(*feature1))[0])\n",
    "    feature1_count= list(list(zip(*feature1))[1])\n",
    "    feature2_keys= list(list(zip(*feature2))[0])\n",
    "    feature2_count= list(list(zip(*feature2))[1])\n",
    "    if feature3:\n",
    "        feature3_keys= list(list(zip(*feature3))[0])\n",
    "        feature3_count= list(list(zip(*feature3))[1])\n",
    "    #print(feature_keys1,feature1_count)\n",
    "    #intersect and find common_feature_keys\n",
    "    common_feature_keys = list(set(feature1_keys) & set(feature2_keys) & set(feature3_keys))\n",
    "    set_diff_operations(feature1_keys,feature2_keys,feature3_keys)\n",
    "    count=[]\n",
    "    feature2_count=[]\n",
    "    feature3_count=[]\n",
    "    for feature in common_feature_keys:\n",
    "        feature1_count=[]\n",
    "        feature1_count.append(feature)\n",
    "        feature1_count.append([count for word,count in feature1 if word == feature][0])\n",
    "        feature1_count.append([count for word,count in feature2 if word == feature][0])\n",
    "        feature1_count.append([count for word,count in feature3 if word == feature][0])\n",
    "        count.append(feature1_count)\n",
    "    plot_df = pd.DataFrame(count, columns =[feature_name, 'WSB', 'Stocks','Investing'], dtype = int)\n",
    "     #plot\n",
    "    plot_df.plot(figsize=(13,6),kind='bar',x=feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bag_of_words(vocab_file,bow_column,n=10):\n",
    "    vocab_file_dict={}\n",
    "    with open('data/wsb_spacy/dictionary_'+vocab_file+'_group_title_selftext.json', 'r') as fp:\n",
    "        vocab_file_dict= json.load(fp)\n",
    "        \n",
    "    vocab= [(pos,word) for word,pos in vocab_file_dict.items() if pos in np.sort(list(list(zip(*bow_column[:n]))[0])).tolist()]\n",
    "    counts =[(index,count) for index,count in bow_column[:n]] \n",
    "    #print(vocab)\n",
    "    #print(counts)\n",
    "    lst=[]\n",
    "    for key,value in vocab:    \n",
    "        val= [pos for word,pos in counts if word == key][0]    \n",
    "        lst.append((value,val))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bag_of_words_(vocab_file,bow_column,n=10):\n",
    "    vocab_file_dict={}\n",
    "    with open('data/wsb_spacy/dictionary_'+vocab_file+'_group_title_selftext.json', 'r') as fp:\n",
    "        vocab_file_dict= json.load(fp)\n",
    "    #print(bow_column[0][:n],bow_column[0][:n])\n",
    "    return list(\n",
    "                zip(\n",
    "                    [word for word,pos in vocab_file_dict.items() if pos in list(list(zip(*bow_column[0][:n]))[0])],              \n",
    "                    list(list(zip(*bow_column[0][:n]))[1])\n",
    "                )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(wsb_combined['bow'][0])[:5] #bag_of_words_investing =map_bag_of_words(\"investing\",eval(investing_all['bow'][0],5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_combined= pd.read_csv('data/wsb_spacy/wsb_all_userBased_normalized.csv',sep=';')\n",
    "stocks_all= pd.read_csv('data/wsb_spacy/stocks_all_userBased_normalized.csv',sep=';')\n",
    "investing_all= pd.read_csv('data/wsb_spacy/investing_all_userBased_normalized.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_wsb =map_bag_of_words(\"wsb\",eval(wsb_combined['bow'][0]),10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_stocks =map_bag_of_words(\"stocks\",eval(stocks_all['bow'][0]),10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_investing =map_bag_of_words(\"investing\",eval(investing_all['bow'][0]),10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wsb_spacy/dictionary_stocks_bow.json', 'w') as fp:\n",
    "    json.dump(bag_of_words_stocks, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wsb_spacy/dictionary_investing_bow.json', 'w') as fp:\n",
    "    json.dump(bag_of_words_investing, fp)\n",
    "with open('data/wsb_spacy/dictionary_wsb_bow.json', 'w') as fp:\n",
    "    json.dump(bag_of_words_wsb, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_wsb={}\n",
    "with open('data/wsb_spacy/dictionary_wsb_bow.json', 'r') as fp:\n",
    "    bag_of_words_wsb= json.load(fp)\n",
    "bag_of_words_stocks ={}\n",
    "with open('data/wsb_spacy/dictionary_stocks_bow.json', 'r') as fp:\n",
    "    bag_of_words_stocks= json.load(fp)\n",
    "bag_of_words_investing ={}\n",
    "with open('data/wsb_spacy/dictionary_investing_bow.json', 'r') as fp:\n",
    "    bag_of_words_investing= json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_diff(feature_name,feature1,feature2,feature3=[],from_=0,till_=10,diff=2): #input parameters: list of tuples\n",
    "    #seperate feature_keys and feature_counts\n",
    "    feature1_keys= list(list(zip(*feature1[from_:till_]))[0])\n",
    "    feature1_count= list(list(zip(*feature1[from_:till_]))[1])\n",
    "    feature2_keys= list(list(zip(*feature2[from_:till_]))[0])\n",
    "    feature2_count= list(list(zip(*feature2[from_:till_]))[1])\n",
    "    if feature3:\n",
    "        feature3_keys= list(list(zip(*feature3[from_:till_]))[0])\n",
    "        feature3_count= list(list(zip(*feature3[from_:till_]))[1])\n",
    "    #print(feature_keys1,feature1_count)\n",
    "    #intersect and find common_feature_keys\n",
    "   # common_feature_keys = list(set(feature1_keys) & set(feature2_keys) & set(feature3_keys))\n",
    "    common_feature_keys = list((set(feature1_keys) & set(feature2_keys)) & (set(feature1_keys) & set(feature3_keys)))\n",
    "   # print(common_feature_keys)\n",
    "    #set_diff_operations(feature1_keys,feature2_keys,feature3_keys)\n",
    "    count=[]\n",
    "    feature2_count=[]\n",
    "    feature3_count=[]\n",
    "    for feature in common_feature_keys:\n",
    "        count1= [count for word,count in feature1 if word == feature][0]\n",
    "        count2= [count for word,count in feature2 if word == feature][0]\n",
    "        count3= [count for word,count in feature3 if word == feature][0]\n",
    "        if (\n",
    "            ((count1 > (3*(count3+count2)) )and (count1 >=0.25)) \n",
    "             or ((count2 > (3*(count1+count3)) )and (count2 >=0.25 ))\n",
    "             or ((count3 > (3*(count1+count2)) )and (count3 >=0.25 ))\n",
    "           ):\n",
    "            if feature not in ['#','(',')','[',']','?','&','-','10','m','amp;#x200b','\\n\\n ','  ','/','%',':','' '','12','4','\\n ','’','e','!','ios_app&amp;utm_name',\n",
    "                               'share&amp;utm_medium','i.e','iossmf','“','+0.2','hi','13.5','.hello', 'search?sort','}','/u','.i','.why','.it','n','ya',\n",
    "                              '{','s3','ya','cuz','\\xa0\\n ','225','q','800','quote.ashx?t','ishare','\"','1',' ','       ','   ','.','     ','rh',\n",
    "                              '️','=','\\xa0 ','amp','r','\\t ',' \\n ','100','...','*','2','20','—','finally','iv','come','worth','at&amp;t','de','edit','🏻','mr','abt',\n",
    "                              'barely','speak','luck','assignment',',','330','like','good','thank',\"'\",'5','23','ba','way','50','200','9','           ',\n",
    "                              '0','130']:\n",
    "                feature1_count=[]\n",
    "                feature1_count.append(feature)\n",
    "                feature1_count.append([count for word,count in feature1 if word == feature][0])\n",
    "                feature1_count.append([count for word,count in feature2 if word == feature][0])\n",
    "                feature1_count.append([count for word,count in feature3 if word == feature][0])\n",
    "                count.append(feature1_count)\n",
    "    if count:\n",
    "        print(count)\n",
    "        plot_df = pd.DataFrame(count, columns =[feature_name, 'WSB', 'Stocks','Investing'], dtype = int)        \n",
    "         #plot\n",
    "        plot_df.plot(figsize=(15,15),kind='barh',x=feature_name,title=feature_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (25*(count3+count2)) )and (count1 >=0.25)) \n",
    "  #           or ((count2 > (25*(count1+count3)) )and (count2 >=0.25 ))\n",
    "   #          or ((count3 > (25*(count1+count2)) )and (count3 >=0.25))\n",
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (25*(count3+count2)) )and (count1 >=0.50)) \n",
    "  #           or ((count2 > (25*(count1+count3)) )and (count2 >=0.50 ))\n",
    "   #          or ((count3 > (25*(count1+count2)) )and (count3 >=0.50))\n",
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (15*(count3+count2)) )and (count1 >=1)) \n",
    "  #           or ((count2 > (15*(count1+count3)) )and (count2 >=1 ))\n",
    "   #          or ((count3 > (15*(count1+count2)) )and (count3 >=1 ))\n",
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (8*(count3+count2)) )and (count1 >=1)) \n",
    "  #           or ((count2 > (8*(count1+count3)) )and (count2 >=1 ))\n",
    "   #          or ((count3 > (8*(count1+count2)) )and (count3 >=1 ))\n",
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#  ((count3 > (5*(count1+count2)) )and (count3 >=1 and count3 <5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count3 >=5 and count3 <=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count3 >10 and count3 <15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count3 >15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count3 >25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count2 >2 and count2 <6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count2 >1 and count2 <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count2 > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count2 > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count2 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count1 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count1 > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count1 > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)#count1 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('BOW',bag_of_words_wsb,bag_of_words_stocks,bag_of_words_investing,0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (2*(count3+count2)) )and (count1 >=0.10)) \n",
    "  #           or ((count2 > (2*(count1+count3)) )and (count2 >=0.10 ))\n",
    "   #          or ((count3 > (2*(count1+count2)) )and (count3 >=0.10 ))\n",
    "plot_features_diff('verbs',eval(wsb_combined['verbs'].tolist()[0])\n",
    "              ,eval(stocks_all['verbs'].tolist()[0])\n",
    "              ,eval(investing_all['verbs'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (2*(count3+count2)) )and (count1 >=0.10)) \n",
    "  #           or ((count2 > (2*(count1+count3)) )and (count2 >=0.10 ))\n",
    "   #          or ((count3 > (2*(count1+count2)) )and (count3 >=0.10 ))\n",
    "plot_features_diff('nouns',eval(wsb_combined['nouns'].tolist()[0])\n",
    "              ,eval(stocks_all['nouns'].tolist()[0])\n",
    "              ,eval(investing_all['nouns'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (2*(count3+count2)) )and (count1 >=0.10)) \n",
    "  #           or ((count2 > (2*(count1+count3)) )and (count2 >=0.10 ))\n",
    "   #          or ((count3 > (2*(count1+count2)) )and (count3 >=0.10 ))\n",
    "plot_features_diff('Adjectives',eval(wsb_combined['adjectives'].tolist()[0])\n",
    "              ,eval(stocks_all['adjectives'].tolist()[0])\n",
    "              ,eval(investing_all['adjectives'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#((count1 > (2*(count3+count2)) )and (count1 >=0.25)) \n",
    "      #       or ((count2 > (2*(count1+count3)) )and (count2 >=0.25 ))\n",
    "       #      or ((count3 > (2*(count1+count2)) )and (count3 >=0.25 ))\n",
    "plot_features_diff('Adjectives',eval(wsb_combined['adjectives'].tolist()[0])\n",
    "              ,eval(stocks_all['adjectives'].tolist()[0])\n",
    "              ,eval(investing_all['adjectives'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #((count1 > (1*(count3+count2)) )and (count1 >=0.25)) \n",
    " #            or ((count2 > (1*(count1+count3)) )and (count2 >=0.25 ))\n",
    " #            or ((count3 > (1*(count1+count2)) )and (count3 >=0.25 ))\n",
    "plot_features_diff('Adjectives',eval(wsb_combined['adjectives'].tolist()[0])\n",
    "              ,eval(stocks_all['adjectives'].tolist()[0])\n",
    "              ,eval(investing_all['adjectives'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('Verbs',eval(wsb_combined['verbs'].tolist()[0])\n",
    "              ,eval(stocks_all['verbs'].tolist()[0])\n",
    "              ,eval(investing_all['verbs'].tolist()[0]),0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_diff('Nouns',eval(wsb_combined['nouns'].tolist()[0])\n",
    "              ,eval(stocks_all['nouns'].tolist()[0])\n",
    "              ,eval(investing_all['nouns'].tolist()[0]),0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wsb_bow = []\n",
    "unique_stocks_bow = []\n",
    "unique_investing_bow = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def set_diff_operations_by_feature(feature1_keys,feature2_keys,feature3_keys,feature_name):\n",
    "    #print(feature1_keys)\n",
    "    #special_characters = \"\"!@$%^&*()-+?_=,<>/\"\"\n",
    "    regex = re.compile('[@_!#%^&*;\\\\ \\-()<>?/\\|}{~:^0-9\\\\\\]') \n",
    "    unique_wsb_bow =list((set(feature1_keys) - set(feature2_keys)) & (set(feature1_keys) - set(feature3_keys)))\n",
    "    unique_stocks_bow = list((set(feature2_keys) - set(feature1_keys)) & (set(feature2_keys) - set(feature3_keys)))\n",
    "    unique_investing_bow = list((set(feature3_keys) - set(feature2_keys)) & (set(feature3_keys) - set(feature1_keys)))\n",
    "    \n",
    "    print(\"Unique in WSB:\",)# unique_wsb_bow[:10])\n",
    "    global_dict = dict([item for item in eval(wsb_combined[feature_name].tolist()[0])[:10000] if (item[0] in unique_wsb_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in WSB:\", len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])\n",
    "    #print([item for item in bag_of_words_wsb if item[0] in unique_wsb_bow][:10])\n",
    "    print()\n",
    "   \n",
    "    \n",
    "    print(\"Unique in Stocks:\",)# unique_stocks_bow[:10])\n",
    "    global_dict = dict([item for item in eval(stocks_all[feature_name].tolist()[0])[:10000] if (item[0] in unique_stocks_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in Stocks:\",len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])\n",
    "    print()\n",
    "  \n",
    "  \n",
    "   \n",
    "    print(\"Unique in Investing:\",)# unique_investing_bow[:10])\n",
    "    global_dict = dict([item for item in eval(investing_all[feature_name].tolist()[0])[:10000] if (item[0] in unique_investing_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in Investing:\", len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in WSB:\n",
      "Unique in WSB: 1617\n",
      "('🙌', '✋', '🤚', '🍌', 'wsbvotebot', 'shitposting', '🦍', 'valhalla', 'cuck', 'retards', 'cock', 'guh', 'contentguide', '🐂', '\\u200d', 'fuckery', 'sld', 'tard', 'retardation', 'degen', 'img', '👊', 'shitron', 'cum', 'stimmy', '💦', '🍆', 'comrade', 'europoor', 'chad', 'dildo', '🏳', 'faggot', 'gainz', 'tendieman', 'moooooon', 'og', '🍋', 'penis', '🖕', 'hodle', 'impersonator', 'fucking', '👐', 'harambe', 'mooning', 'shitadel', 'cucke', '🛰', '🥵', 'scum', 'opad', 'cheek', 'uranus', '🌌', 'chimp', '🇸', '🏿', 'popeye', 'prophet', 'tribute', 'trendie', 'chromosome', '👎', 'mascot', '😍', 'wsber', '🧑', 'lube', '❤', '👩', 'mooooooon', '🧻', '🐵', 'lord', '🎶', 'bionano', 'tothemoon', 'despac', '🤯', '🍺', 'monke', 'robbinghood', 'squoze', '😤', '✊', 'butthole', '🌚', '💩', 'eater', 'caption', 'papa', 'tendy', '😡', 'paperhand', 'dipshit', '👆', '🆘', '🍿', 'zoo')\n",
      "\n",
      "Unique in Stocks:\n",
      "Unique in Stocks: 1266\n",
      "('streetinsider', 'psychomarket', 'kulr', 'aaii', 't.b.a', 'ichimoku', 'wtd', 'stocks', 'quarterly', 'rivn', 'decliner', 'patterns', 'map.ashx', 'futures.ashx', 'more.html', 'pfmt', 'antony', 'sro', 'opy', 'jac', 'choppiness', 'nig', 'ebita', 'almanac', 'fiserv', 'assertio', 'mmbtu', 'bector', 'svb', 'venom', 'emanuel', 'observance', 'hyperloop', 'truist', 'snca', 'ord', 'sephora', 'aquabounty', 'centene', 'dgly', 'lightspeed', 'leerink', 'bund', 'tler', 'cryptoswap', 'rps', 'sodium', 'boxl', 'nrgu', 'douyu', 'thcx', 'eur€', 'gbp£', 'decile', 'margenza', 'crnc', 'agilent', 'tza', 'brainard', 'kodal', 'cfra', 'stockstobuy', 'stpk', 'pinterest', 'ddic', 'btcs', 'jpy¥', 'syta', 'hesitancy', 'boockvar', 'bamlanivimab', 'xpo', 'naov', 'rrr', 'entain', 'cochlea', 'lam', 'niu', 'grove', 'shrug', 'mfst', 'clis', 'veev', 'jmp', 'pearson', 'jgbs', 'sulfur', 'industrial', 'hess', 'pbt', 'switchback', 'dickinson', 'bottoming', 'abbott', 'greenpower', 'plateaue', 'gardening', 'gohealth', 'tpii', 'pdsb')\n",
      "\n",
      "Unique in Investing:\n",
      "Unique in Investing: 1475\n",
      "('\\u200e', 'fundrise', 'gundlach', 'ukraine', 'criteo', 'advancethank', 'duplex', 'defer', 'halving', 'referal', 'admiral', 'expat', '🛢', 'eharth', 'baos', 'backt', 'convalescent', 'grantham', 'acorns', 'cashback', 'vtsmx', 'vfinx', 'bndx', 'heloc', 'chainlink', 'crowdfunding', 'lifestrategy', 'sche', 'tiaa', 'vgslx', 'budgeting', 'trp', 'yieldstreet', 'σ', 'thestreet', 'vigax', 'stc', 'zimmer', 'thrift', 'swisx', 'vea', 'fiduciary', 'reinsurance', 'misclassification', 'goosehead', 'fxnax', 'scottrade', 'withdraw', 'vffvx', 'novc', 'wefunder', 'staking', 'ivig', 'muni', 'jhv', 'ytm', 'gic', 'vsmax', 'ror', 'harmonychain', 'resp', 'qwo', 'eib', 'indexer', 'iras', 'edv', 'electra', 'siegel', 'agi', 'pimco', 'schx', 'hane', 'invesment', 'epipen', 'renting', 'youinv', 'vbtlx', 'lighthizer', 'zacks', 'cyxtera', 'cellulite', 'mmf', 'eurusd', 'lakh', 'nw', 'marcus', 'hy', 'hoa', 'compounder', 'swagx', 'vnqi', 'confidentiality', 'cfa', 'bangalore', 'tdf', 'impetus', 'sri', 'startengine', 'annualise', 'vbk')\n"
     ]
    }
   ],
   "source": [
    "set_diff_operations_by_feature(set(list(list(zip(*eval(wsb_combined['nouns'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(stocks_all['nouns'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(investing_all['nouns'].tolist()[0])[:10000]))[0])),\n",
    "                              'nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in WSB:\n",
      "Unique in WSB: 810\n",
      "('BFS', 'LIDR', 'HOWL', 'RCON', 'TPST', 'MORN', 'ARMK', 'BME', 'RETA', 'OSH', 'HLTH', 'GATO', 'CCRN', 'EDD', 'HIPO', 'MTC', 'MOXC', 'STG', 'OGE', 'ATIP', 'AAIC', 'ECVT', 'DRRX', 'GROY', 'ELA', 'YGMZ', 'MOH', 'PAM', 'HGH', 'CASI', 'BALY', 'MFIN', 'BZ', 'CABO', 'GPK', 'LOB', 'DTF', 'CLBT', 'INFN', 'NGS', 'NXTP', 'OCA', 'ISIG', 'BIOX', 'ZIXI', 'FFA', 'PRCH', 'FREE', 'BKI', 'MMYT', 'VIEW', 'CHH', 'MUR', 'DXLG', 'CODA', 'WNS', 'OBLG', 'OCUL', 'TMQ', 'WLK', 'KORE', 'GPRE', 'AXLA', 'PHIO', 'ADEX', 'MRC', 'RERE', 'AMPS', 'FTCI', 'VWE', 'PRQR', 'EGHT', 'ZVIA', 'VWTR', 'SOHU', 'NPTN', 'NPO', 'RNAZ', 'CFLT', 'INDP', 'GAMB', 'CVLT', 'KEP', 'VEV', 'TBLA', 'PKX', 'VIRI', 'HTGM', 'FOR', 'SWIR', 'HBM', 'DBD', 'OXM', 'ARCO', 'PLAN', 'ARE', 'NHTC', 'MBUU', 'AM', 'MCI')\n",
      "\n",
      "Unique in Stocks:\n",
      "Unique in Stocks: 271\n",
      "('BPT', 'BRPM', 'NMIH', 'BOMN', 'EVAX', 'HCSG', 'ENLC', 'SMIT', 'CJJD', 'ATRO', 'KALV', 'IMRA', 'BMTX', 'EXFY', 'PBT', 'NLTX', 'TTP', 'OESX', 'ELMD', 'RMNI', 'UTSI', 'BCDA', 'CTMX', 'ICLR', 'JXN', 'SABS', 'VECO', 'DBTX', 'HUIZ', 'MHI', 'KYMR', 'SHG', 'ASUR', 'REPX', 'XFOR', 'OPNT', 'HLIT', 'MNR', 'TTMI', 'RVMD', 'STRO', 'PBA', 'SWM', 'EMX', 'AXU', 'BBGI', 'STXB', 'YMAB', 'TPH', 'PRDO', 'VLDRW', 'SNOA', 'CLGN', 'VGFC', 'ORIC', 'PNNT', 'RSSS', 'DIOD', 'CMPI', 'SRRA', 'MCMJ', 'NBIX', 'MNTV', 'ROG', 'ARKR', 'OCSL', 'OTTR', 'USAP', 'MHH', 'AOSL', 'ESRT', 'VMD', 'AKTX', 'UAN', 'IBA', 'WBX', 'SBET', 'LCAP', 'ROCC', 'NMCO', 'MEC', 'KRUS', 'POWI', 'CNXC', 'GHAC', 'PAVMZ', 'ECF', 'BCV', 'BREZ', 'AMSF', 'KIND', 'FWBI', 'IROQ', 'JANX', 'FLMN', 'ALYA', 'SLAMU', 'BCSF', 'IREN', 'MATW')\n",
      "\n",
      "Unique in Investing:\n",
      "Unique in Investing: 114\n",
      "('MKL', 'CTHR', 'JPC', 'SBRA', 'BNR', 'PDCE', 'FDUS', 'PICC', 'MGR', 'ENV', 'VNDA', 'TIXT', 'LPLA', 'HRZN', 'BLX', 'BKT', 'MNP', 'NATH', 'SVT', 'AMPL', 'KOR', 'SVVC', 'HNNA', 'AHPA', 'JHG', 'RQI', 'ENVIU', 'AHPAW', 'CTLP', 'EWBC', 'ZT', 'VRTS', 'AGD', 'ANZU', 'TMP', 'RHP', 'RJF', 'FIF', 'EXG', 'ARTW', 'LIAN', 'THFF', 'CVCY', 'HBANP', 'CDOR', 'IEAWW', 'CQP', 'VERX', 'MQY', 'MYD', 'CEN', 'GSLD', 'LABP', 'NMZ', 'PGP', 'IIF', 'ALACW', 'PGSS', 'NBSE', 'HYB', 'PCM', 'LEGA', 'TCPC', 'VGI', 'NWE', 'NVEI', 'RVT', 'ELTK', 'NTIP', 'KEQU', 'HFRO', 'PCSA', 'NKG', 'EPAC', 'UHT', 'FLXS', 'VMO', 'FFC', 'GGN', 'ACR', 'GNT', 'CIAN', 'LDP', 'AWF', 'BFK', 'OSG', 'IMKTA', 'AIF', 'ESGRO', 'IVAC', 'MCA', 'IDW', 'PHK', 'LTRY', 'AUDC', 'GBL', 'EVV', 'NVST', 'ETW', 'PIII')\n"
     ]
    }
   ],
   "source": [
    "set_diff_operations_by_feature(set(list(list(zip(*eval(wsb_combined['tickers'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(stocks_all['tickers'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(investing_all['tickers'].tolist()[0])[:10000]))[0])),\n",
    "                              'tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in WSB:\n",
      "Unique in WSB: 2332\n",
      "('nigger', '✋', 'hedgie', 'tard', 'irnt', '🧻', 'donny', '🌚', '💪', 'deepfuckingvalue', 'yachty', 'controlthenarrative', '🌔', '🤲', '🦍', 'tendie', '😌', 'chad', '😘', 'gamestahp', '🛰', 'shitpost', 'nutshell', '👋', 'milky', '🥜', 'booty', 'despac', 'motherfucke', 'deez', 'moooon', 'wrinkly', 'squoze', 'unbreakable', 'fu', 'retarted', 'nude', 'citadel', '😂', 'fuk', 'linus', 'wendys', 'fuckery', 'fibo', 'gaybear', 'finna', 'sellin', 'unch', 'cucke', 'slv', 'faggy', 'retarte', 'mooon', 'hoooold', '☄', '✊', 'puny', '🐜', 'blonde', '🇷', 'johnny', 'mumble', 'yell', 'pow', '🤘', 'sleazy', 'fucktards', 'paperhand', 'ded', 'mammal', 'tarde', 'puss', 'lolz', 'intergalactic', 'adorable', '˚', 'refuel', '🚧', 'nonfactual', 'ez', '😢', '🏼', 'europoor', 'spamme', 'puppy', 'endgame', '\\U0001f9a7', 'runnin', 'dusty', 'tingle', 'haired', 'shareslatest', 'frontdoor', 'bitch', 'boiz', '🇵', 'dipshit', 'palantard', 'disrespect', 'apetard')\n",
      "\n",
      "Unique in Stocks:\n",
      "Unique in Stocks: 1890\n",
      "('rsmb', 'lpl', 'ytd', 'elsese', 'preveceutical', 'smid', 'stovall', 'kimbal', 'macrogenic', 'sweetgreen', 'netapp', 'genprex', 'ghoul', 'cntg', 'sclc', 'crest', 'arconic', 'vwdry', 'termite', 'oncoprex', 'kinematic', 'ttnp', 'upmove', 'arkf', 'nsr', 'chi\\xadnese', 'romulus', 'bigtoken', 'blake', 'ssl', 'jakks', 'varian', 'arggy', 'iitu', 'axp', 'fortitude', 'antibacterial', 'okayish', 'fost', 'inoculate', 'axsm', 'scenic', 'bcc', 'electrolyser', 'facetime', 'dutchy', 'jakk', 'evs', 'elec\\xadtric', 'intracutaneous', 'uniden', 'uroshield', 'salesforce', 'randolph', 'dampened', 'fngu', 'shepard', 'vrar', 'underpin', 'nrgu', 'reinstate', 'misuse', 'cnty', 'lucent', 'wheel', 'nand', 'pnm', 'resent', 'trevali', 'kraneshare', 'tnp', 'icasino', 'sibanye', 'igov', 'omniverse', 'avis', 'garmin', 'weta', 'zerodha', 'i´ll', 'grst', 'momentive', 'fastly', 'krosby', 'china´s', 'mcdonald´s', 'prehospital', 'haj', 'tbtf', 'nxo', 'pstl', 'quiverquant', 'parcelpal', 'tali', 'inese', 'wider', 'iminent', 'conflict', 'seadream', 'chicagoland')\n",
      "\n",
      "Unique in Investing:\n",
      "Unique in Investing: 2165\n",
      "('spitballin', 'referal', 'fundrise', 'sortino', 'kinh', 'raac', 'dumping', 'precipitate', 'intellinetic', 'doubleline', 'nonresident', 'damodaran', 'qapital', 'vygg', 'dnenjejenfnfnjfjennccnndjfnfjdjjfjcjfjvjfjfjfjfjjfjdnfjvjfjdncncndnncnfnencndndncndndbcndn', 'tna', 'mpt', 'union', 'oppy', 'smartmetric', 'enzolytic', 'simplest', 'daedalus', 'maiar', 'iff', 'intrastate', 'youinvest', 'nct', 'nol', 'cfd', 'transmedic', 'misplace', 'torq', 'untaxed', 'eligibility', 'correlated', 'investous', 'webdollar', 'vgt', 'bogle', 'sei', 'lgfvs', 'retrieve', 'state', 'counterpartie', 'ugld', 'unsubsidized', 'usa', 'decentral', 'paradoxical', 'inital', 'cryptographic', 'triplex', 'enterprising', 'overstate', 'pimco', 'ied', 'lic', 'eafe', 'utkarsh', 'mezzanine', 'vstax', 'jfe', 'fba', 'parliamentary', 'beard', 'indf', 'osram', 'emailed', 'backt', 'schb', 'bric', 'lex', 'imf', 'vwigx', 'hurst', 'fibrous', 'duramed', 'ucan', 'vfinx', 'wisebanyan', 'udemy', 'saved', 'ezy', 'elrond', 'transatlantic', 'fcntx', 'specified', 'agency', 'qtec', 'understanding', 'urothelial', 'lankan', 'accomodative', 'hedgeable', 'lacklustre', 'nestlé', 'reccomended', 'etv', 'noninterest')\n"
     ]
    }
   ],
   "source": [
    "set_diff_operations_by_feature(set(list(list(zip(*eval(wsb_combined['adjectives'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(stocks_all['adjectives'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(investing_all['adjectives'].tolist()[0])[:10000]))[0])),\n",
    "                              'adjectives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in WSB:\n",
      "Unique in WSB: 2914\n",
      "('🌈', '\\U0001f90e', '⣿', 'ape', '�', 'goooooo', '🏳', '🌝', 'tard', '🍆', 'diamondhand', 'revs', 'dumbass', 'holdin', 'handed', '🧻', 'cum', 'gooooooo', 'dfv', 'saturn', 'fucken', '🤠', \"yolo'd\", 'pluto', 'yolo’d', 'snort', '🤚', 'masturbate', 'nut', 'gay', 'shitpost', 'coo', 'art', '👐', '🖕', 'boyz', 'deepfuckingvalue', 'expr', 'cunt', 'shitadel', 'fke', 'tendieland', 'gang', 'viking', 'motherfucker', 'paperhande', 'dipshit', 'faggot', '🖐', '🗑', 'immortalize', 'begat', 'emoji', 'flaire', 'pamp', '⠁', 'done', 'clench', 'cream', 'boutta', 'yo', 'colorize', 'cock', '🤤', 'squeezy', 'ass', 'cya', 'shortie', 'trendie', '🐵', 'goodnight', 'shithead', '🍋', '😫', '🌚', '🍰', 'mfer', 'reload', 'cucke', 'unpin', '💪', 'commentposte', 'leaving', 'slurp', 'cumme', 'united', 'tking', '🥰', 'rtrd', '✨', 'boyfriend', 'vxa', 'valhalla', '🌘', 'billboard', 'saiyan', 'mod', '🐂', 'holdtheline', 'hooooold')\n",
      "\n",
      "Unique in Stocks:\n",
      "Unique in Stocks: 2431\n",
      "('expensify', 'cibr', 'fredsavesgrandma', 'dbk', 'quantafuel', 'transat', 'rps', 'bluecruise', '🍬', 'desjardin', 'encouraged', 'candlestick', 'welltower', 'dupuytren', 'culper', 'nmih', 'eri', 'cbdmd', 'brokered', 'ttnp', 'citrix', 'scanner', 'rds.b', 'unleverage', 'disintermediate', 'sweetgreen', 'izrl', 'matched', 'hypothecate', 'pstl', 'redshift', 'mclean', 'reiss', 'versed', 'shifting', 'gan', 'enb', 'pbf', 'science', 'accompanying', 'bcs', 'sabr', 'subreddit', 'copperstate', 'qts', 'énorme', 'suited', 'bioc', 'misappropriate', 'carsten', 'scaremongere', 'overanalyze', 'rugged', 'suncor', 'eweb', 'lkncy', 'tgna', 'slf', 'yearend', 'lndc', 'maintains', 'cochlea', 'addot', 'sunnova', 'irk', 'msd', 'scr', 'opiate', 'holds', 'court', 'nrs', 'petropavlovsk', 'heating', 'trying', 'teardown', 'weakness', 'brll', 'outweight', 'purchased', 'hapen', 'launches', 'diversifie', 'cvs', 'triterras', 'resuming', 'avangrid', 'partecipate', 'reaccelerate', 'faamg', \"ipo'e\", 'fss', 'utz', 'hunk', 'appetite', 'serum', 'wolfpack', 'interoperate', 'graphite', 'discussion', 'skoda')\n",
      "\n",
      "Unique in Investing:\n",
      "Unique in Investing: 2617\n",
      "('investorsobserver', 'fbgrx', 'vanguard', 'alkerme', 'vfiax', 'dalio', 'buckling', 'renting', 'coupa', 'vea', 'piotroski', 'fundrise', 'dfa', 'fncmx', 'recharacterize', 'donc', 'bushveld', 'unifire', 'roi', 'haggle', 'advantaged', 'smme', 'vlcax', 'npv', 'gullible', 'brokerage', 'presage', 'tiaa', 'tab', 'परिणाम', 'liveramp', 'cpr', 'dacc', 'sso', 'freelance', 'liquid', 'premade', 'vffvx', 'bulletshare', 'freelancing', 'laundering', 'vwinx', 'government', 'cannergrow', 'ijs', 'mispricing', 'friel', 'hsa', 'listed', 'groundfloor', 'terabase', 'cysec', 'vixm', 'multicloud', 'clint', 'misstate', 'vcs', 'rothira', 'neumann', 'schd', 'behring', 'persue', 'emigrate', 'crowdfunding', 'perry', 'borrower', 'substack', 'unidefi', 'schm', 'suburb', 'outsourcing', 'vimax', 'blog', 'witholde', 'studentcoin', 'vxf', 'icici', 'diy', 'vitax', 'usdt', 'fid', 'rerun', 'personalise', 'vclt', 'ota', 'pabrai', 'gdpr', 'knoe', 'domiciled', 'wealthfront', 'property', 'biocept', 'disclaimer', 'puzzled', 'roundup', 'misclassifye', 'svxy', 'может', 'cochlear', 'troubled')\n"
     ]
    }
   ],
   "source": [
    "set_diff_operations_by_feature(set(list(list(zip(*eval(wsb_combined['verbs'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(stocks_all['verbs'].tolist()[0])[:10000]))[0])),\n",
    "                     set(list(list(zip(*eval(investing_all['verbs'].tolist()[0])[:10000]))[0])),\n",
    "                              'verbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_diff_operations_(feature1_keys,feature2_keys,feature3_keys):\n",
    "    #special_characters = \"\"!@$%^&*()-+?_=,<>/\"\"\n",
    "    regex = re.compile('[@_!#%^&*;\\\\ \\-()<>?/\\|}{~:^0-9\\\\\\]') \n",
    "    unique_wsb_bow =list((set(feature1_keys) - set(feature2_keys)) & (set(feature1_keys) - set(feature3_keys)))\n",
    "    unique_stocks_bow = list((set(feature2_keys) - set(feature1_keys)) & (set(feature2_keys) - set(feature3_keys)))\n",
    "    unique_investing_bow = list((set(feature3_keys) - set(feature2_keys)) & (set(feature3_keys) - set(feature1_keys)))\n",
    "   \n",
    "    print(\"Unique in WSB:\",)# unique_wsb_bow[:10])\n",
    "    global_dict = dict([item for item in bag_of_words_wsb if (item[0] in unique_wsb_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in WSB:\", len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])\n",
    "    #print([item for item in bag_of_words_wsb if item[0] in unique_wsb_bow][:10])\n",
    "    print()\n",
    "   \n",
    "    \n",
    "    print(\"Unique in Stocks:\",)# unique_stocks_bow[:10])\n",
    "    global_dict = dict([item for item in bag_of_words_stocks if (item[0] in unique_stocks_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in Stocks:\",len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])\n",
    "    print()\n",
    "  \n",
    "  \n",
    "   \n",
    "    print(\"Unique in Investing:\",)# unique_investing_bow[:10])\n",
    "    global_dict = dict([item for item in bag_of_words_investing if (item[0] in unique_investing_bow and regex.search(item[0]) == None)])\n",
    "    print(\"Unique in Investing:\", len(global_dict))\n",
    "    print(list(list(zip(*sorted(global_dict.items(), key=lambda item: item[1], reverse = True)[:100])))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in WSB:\n",
      "Unique in WSB: 1406\n",
      "('👐', '🤲', '✋', 'autistic', '🍌', '🤚', 'autism', '🏽', 'fucker', 'img', 'tard', '🖐', 'bois', 'deepfuckingvalue', 'retards', 'nigger', 'pussy', '🌑', '✊', '🤡', 'bastard', '\\U0001f9a7', 'wsbvotebot', '👏', 'yoloe', '🖕', 'cuck', 'hodle', 'moass', '👋', 'motherfucker', 'chad', '😤', 'pluto', 'shitposting', 'gainz', 'gorilla', 'revs', 'valhalla', 'tattoo', '🛸', '🍆', 'guh', '🥜', '✨', 'stimmy', 'shitron', '💩', 'lad', '🇺', 'cock', 'squoze', '🤝', '🧻', 'dildo', '💦', 'yacht', 'aoc', '️\\u200d', 'hfs', 'fuk', 'shitpost', 'lfg', 'irnt', '👨', 'og', '🇸', 'mnmd', 'bf', '🏳', '🐂', '😈', 'yolo’d', '🤬', '😡', 'eow', 'gamestonk', '🤦', \"yolo'd\", 'goddamn', '👌', '🏻\\u200d', '🐒', 'jerk', 'glorious', 'godspeed', '🍿', 'contentguide', 'monke', 'fuckery', '🌖', 'goooo', 'billboard', 'cum', '🍗', '😳', 'degen', 'wrinkle', 'becky', 'bigly')\n",
      "\n",
      "Unique in Stocks:\n",
      "Unique in Stocks: 771\n",
      "('streetinsider', 'market+check', 'kulr', 'lplresearch.com', 'aggregated', 'tcgyt', 'psychomarket', 'eri', 'unattractive', 'minis', 't.b.a', 'rsmb', 'aaii', 'douyu', 'mrs', 'bector', 'sandler', 'betz', 'expi', 'usd$', 'luman', 'hotly', 'detrick', 'ichimoku', 'stryker', 'mizuho', 'autoremove', 'mtd', 'atr', 'choppy', 'stocks', 'sportradar', 'lunchtime', 'dma', 'bollinger', 'wtd', 'whipsaw', 'keycorp', 'maxn', 'nrgu', 'glsi', 'qtd', 'datadog', 'agtc', 'rivn', 'calloption.asp', 'putoption.asp', 'stockchart', 'decliner', 'dcf.asp', 'patterns', 'stifel', 'pfmt', 'kt', 'chi', 'selectquote', 'lightspeed', 'bepc', 'upbeat', 'xtsx', 'map.ashx', 'groups.ashx', 'truist', 'lly', 'arry', 'akamai', 'netlist', 'xpo', 'futures.ashx', 'sabr', 'srad', 'ddic', 'more.html', 'tmo', 'evbox', 'microchip', 'btig', 'pinter', 'deceleration', 'baird', 'fmci', 'rfp', 'mojo', 'inrg', 'argus', 'vwagy', 'umc', 'rollin', 'hopkins', 'fnko', 'antony', 'hitif', 'panw', '₹', 'hogan', 'cfra', 'upwk', 'canaccord', 'jks', 'autodesk')\n",
      "\n",
      "Unique in Investing:\n",
      "Unique in Investing: 1087\n",
      "('\\u200e', '\\n', 'hsa', 'wealthfront', 'spitballin', 'admiral', '\\n\\n', 'backdoor', 'annuity', 'bogle', 'crowdfunde', 'apy', 'gundlach', 'vtiax', 'fundrise', 'tsp', 'muni', 'rowe', 'backt', 'referal', 'tether', 'domicile', 'hysa', 'moronic', 'acorns', 'bridgewater', 'itot', 'mmmmm', 'dk', 'iras', 'damodaran', 'khosrowshahi', 'vbtlx', 'propane', 'criteo', 'vangaurd', 'cef', 'downpayment', 'gfc', 'sensex', 'swtsx', 'chainlink', 'neumann', 'planner', 'internship', 'irr', 'turkish', 'tcw', 'lifestrategy', 'sso', 'spouse', 'personalfinance', 'tmf', 'vea', 'frugal', 'endowment', 'advancethank', 'duplex', 'expat', 'tiaa', 'halving', 'devaluation', 'havoc', 'structured', 'scholarship', 'isin', 'paywall', 'literacy', 'preservation', 'aia', 'mogo', 'vfinx', 'vanadium', 'periodic', 'parity', 'agg', 'farmland', 'acwi', 'retiree', 'fnilx', '🛢', 'custody', 'primer', 'vtsmx', 'acat', 'citizenship', 'gbtc', 'gic', 'piotroski', 'schf', 'delinquency', 'vb', 'aversion', 'xiv', 'grantham', 'stc', 'rupee', 'deductible', 'fzilx', 'instl')\n"
     ]
    }
   ],
   "source": [
    "set_diff_operations_(set(list(list(zip(*bag_of_words_wsb[:]))[0])),set(list(list(zip(*bag_of_words_stocks[:]))[0])),set(list(list(zip(*bag_of_words_investing[:]))[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GME', 6.728153807591976),\n",
       " ('AMC', 3.3868212664432455),\n",
       " ('BB', 1.305596041552558),\n",
       " ('NOK', 0.8876103565759057),\n",
       " ('RH', 0.8659103779277564),\n",
       " ('TSLA', 0.8167082873030757),\n",
       " ('PLTR', 0.7377992740370779),\n",
       " ('CLOV', 0.3501007250345807),\n",
       " ('NIO', 0.3267761490544844),\n",
       " ('SNDL', 0.313547285065773),\n",
       " ('TD', 0.3079772370705261),\n",
       " ('SPCE', 0.2997382077442234),\n",
       " ('COIN', 0.2953285864146529),\n",
       " ('AMD', 0.2817515944262386),\n",
       " ('HOOD', 0.2561061651147894),\n",
       " ('RKT', 0.2427612584595104),\n",
       " ('AAPL', 0.21641957314865531),\n",
       " ('KIDS', 0.21189390915251718),\n",
       " ('UK', 0.20934097048802905),\n",
       " ('SHIP', 0.1954158504999118),\n",
       " ('NAKD', 0.1828832425106063),\n",
       " ('MSFT', 0.18125864517865928),\n",
       " ('APPS', 0.18033030384611815),\n",
       " ('FAT', 0.1616474345287275),\n",
       " ('EYES', 0.147490229207475),\n",
       " ('AMZN', 0.14679397320806914),\n",
       " ('WISH', 0.1384389012151988),\n",
       " ('AIR', 0.13785868788236058),\n",
       " ('BLUE', 0.13693034654981945),\n",
       " ('BABA', 0.13379719455249306)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(wsb_combined['tickers'].tolist()[0])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CUZ', 0.1312442558880049),\n",
       " ('SELF', 0.13078008522173434),\n",
       " ('SPOT', 0.12729880522470502),\n",
       " ('BOOM', 0.12497795189335215),\n",
       " ('MASS', 0.12439773856051392),\n",
       " ('JOBS', 0.12370148256110806),\n",
       " ('LAND', 0.12149667189632284),\n",
       " ('MVIS', 0.11929186123153762),\n",
       " ('TLRY', 0.1176672638995906),\n",
       " ('GLAD', 0.11708705056675238),\n",
       " ('ATH', 0.11592662390107594),\n",
       " ('DIS', 0.11546245323480538),\n",
       " ('DOW', 0.11488223990196715),\n",
       " ('USA', 0.11453411190226423),\n",
       " ('AKA', 0.11441806923569658),\n",
       " ('LMAO', 0.1141859839025613),\n",
       " ('WKHS', 0.11372181323629073),\n",
       " ('GRAB', 0.11360577056972307),\n",
       " ('EARN', 0.11256138657061428),\n",
       " ('LAW', 0.112329301237479)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(wsb_combined['tickers'].tolist()[0])[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GME', 1.3779400152744876),\n",
       " ('AMC', 0.8498051855591369),\n",
       " ('TSLA', 0.7462493365953426),\n",
       " ('NIO', 0.6983547564495877),\n",
       " ('AMD', 0.5106597802027105),\n",
       " ('AAPL', 0.507423659922592),\n",
       " ('TD', 0.42587342886360396),\n",
       " ('MSFT', 0.4206956364154143),\n",
       " ('PLTR', 0.3922177779503709),\n",
       " ('BABA', 0.32620092423595204),\n",
       " ('AMZN', 0.29772306577090857),\n",
       " ('AIR', 0.280895240314292),\n",
       " ('RH', 0.2647146389136991),\n",
       " ('BB', 0.2485340375131063),\n",
       " ('PLUG', 0.2459451412890114),\n",
       " ('AI', 0.2258811955522763),\n",
       " ('DOW', 0.22393952338420514),\n",
       " ('DKNG', 0.21876173093601545),\n",
       " ('NVDA', 0.21811450687999173),\n",
       " ('FORD', 0.21617283471192056),\n",
       " ('NOK', 0.20322835359144628),\n",
       " ('UBER', 0.20322835359144628),\n",
       " ('DIS', 0.20322835359144628),\n",
       " ('FB', 0.20258112953542257),\n",
       " ('SPCE', 0.20063945736735145),\n",
       " ('BOOM', 0.19675611303120916),\n",
       " ('ROSE', 0.18316440785471114),\n",
       " ('ALOT', 0.17475049512640287),\n",
       " ('SQ', 0.17086715079026057),\n",
       " ('SPOT', 0.16633658239809457)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(stocks_all['tickers'].tolist()[0])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EARN', 0.6918736156657718),\n",
       " ('GME', 0.6770030289037247),\n",
       " ('UK', 0.5932581455595646),\n",
       " ('TD', 0.565082296957791),\n",
       " ('TSLA', 0.5455157354287816),\n",
       " ('APPS', 0.4750761139243479),\n",
       " ('SUM', 0.44455227793909324),\n",
       " ('DOW', 0.4093324671868763),\n",
       " ('PAYS', 0.39289655550250846),\n",
       " ('RENT', 0.3748953188958198),\n",
       " ('JOBS', 0.3678513567453764),\n",
       " ('BLUE', 0.34906745767752745),\n",
       " ('AAPL', 0.3326315459931595),\n",
       " ('USA', 0.32871823368735764),\n",
       " ('SELF', 0.3201089466145935),\n",
       " ('AMD', 0.3028903724690653),\n",
       " ('COIN', 0.2770625112507729),\n",
       " ('WASH', 0.2762798487896125),\n",
       " ('LAW', 0.2747145238672918),\n",
       " ('AMC', 0.2708012115614899),\n",
       " ('MSFT', 0.26610523679452763),\n",
       " ('ROSE', 0.2551479623382824),\n",
       " ('AI', 0.2410600380373956),\n",
       " ('AMZN', 0.235581400809273),\n",
       " ('LAND', 0.23245075096463147),\n",
       " ('PATH', 0.23245075096463147),\n",
       " ('KIDS', 0.23166808850347112),\n",
       " ('AIR', 0.23010276358115034),\n",
       " ('SPOT', 0.22853743865882964),\n",
       " ('NIO', 0.22462412635302775),\n",
       " ('UBER', 0.22384146389186735),\n",
       " ('GDP', 0.22227613896954662),\n",
       " ('TEN', 0.22227613896954662),\n",
       " ('RH', 0.1909696405231316),\n",
       " ('UNIT', 0.18862165313965049),\n",
       " ('MASS', 0.18627366575616933),\n",
       " ('FB', 0.18627366575616933),\n",
       " ('RELY', 0.1847083408338486),\n",
       " ('BOOM', 0.1815776909892071),\n",
       " ('ALOT', 0.18001236606688634),\n",
       " ('MSCI', 0.17375106637760332),\n",
       " ('FORD', 0.17296840391644294),\n",
       " ('PM', 0.17140307899412222),\n",
       " ('PRO', 0.1651417793048392),\n",
       " ('BABA', 0.16357645438251847),\n",
       " ('CD', 0.16279379192135807),\n",
       " ('MET', 0.1612284669990373),\n",
       " ('GE', 0.15418450484859395),\n",
       " ('JOE', 0.14714054269815058),\n",
       " ('RARE', 0.14557521777582982)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(investing_all['tickers'].tolist()[0])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🚀', 48.64311309982454],\n",
       " ['hold', 10.825040150762632],\n",
       " ['☀', 0.02703794131026096],\n",
       " ['🙌', 3.0454237414012386],\n",
       " ['🔥', 0.771335604675127],\n",
       " ['🦍', 2.8880698855355136],\n",
       " ['etf', 0.5299668582144282],\n",
       " ['✋', 0.6139817488094023],\n",
       " ['🤷', 0.1538725758686954],\n",
       " ['👀', 0.2302286504702049],\n",
       " ['🤬', 0.08714804259230033],\n",
       " ['👋', 0.1322886398871137],\n",
       " ['etfs', 0.27861844242891226],\n",
       " ['😍', 0.06359138127906869],\n",
       " ['🦅', 0.013112821322143725]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis=['⏰','🤬','😍','🦅','🙌','✋','🚀','☀','🤷','🔥','👋','👀','🦍','hold','ETF','ETFs','etf','etfs']\n",
    "\n",
    "[item for item in bag_of_words_wsb if item[0] in emojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['etfs', 2.9461639030199476],\n",
       " ['etf', 4.061330951548807],\n",
       " ['hold', 8.588663223434688],\n",
       " ['🚀', 2.0646447387156486],\n",
       " ['🙌', 0.05630849287406314],\n",
       " ['🔥', 0.17928106351856885],\n",
       " ['🤷', 0.024594514128901145],\n",
       " ['👀', 0.06860574993851372],\n",
       " ['🦍', 0.03689177119335171]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_stocks if item[0] in emojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['etf', 7.706877255046216],\n",
       " ['hold', 8.052814062879103],\n",
       " ['etfs', 6.031979588163013],\n",
       " ['🚀', 0.9681534644553843],\n",
       " ['🦍', 0.018783899067849007],\n",
       " ['🔥', 0.16357645438251847],\n",
       " ['🤷', 0.017218574145528256],\n",
       " ['🙌', 0.028175848601773513],\n",
       " ['👀', 0.02895851106293389]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_investing if item[0] in emojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['🚀', 48.64311309982454],\n",
       " ['buy', 19.718666159173402],\n",
       " ['gme', 15.814990855837873],\n",
       " ['high', 5.411533712715491],\n",
       " ['sell', 10.273373313900054],\n",
       " ['moon', 6.393602799877458],\n",
       " ['dd', 2.994481010778043],\n",
       " ['buying', 0.8832007352463354],\n",
       " ['cut', 0.6366100687900927],\n",
       " ['short', 12.255614144208543],\n",
       " ['growth', 2.188912819465461],\n",
       " ['ape', 4.38826947892201],\n",
       " ['💎', 8.408451619491455],\n",
       " ['long', 5.400625702058133],\n",
       " ['etf', 0.5299668582144282],\n",
       " ['squeeze', 4.097002385837225],\n",
       " ['dividend', 0.7158672100557933],\n",
       " ['etfs', 0.27861844242891226],\n",
       " ['apes', 0.014505333320955448],\n",
       " ['options', 0.019263082650228837]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_lst=['buying','gme','apes','high','short','buy','🚀','💎','long','dd','sell','growth','squeeze','moon','options','cut','call','dividend','dividends', 'etf', 'etfs','apes','ape']\n",
    "[item for item in bag_of_words_wsb if item[0] in bow_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['etfs', 2.9461639030199476],\n",
       " ['etf', 4.061330951548807],\n",
       " ['gme', 3.289192652712516],\n",
       " ['buy', 26.63974214593608],\n",
       " ['cut', 1.2679119257504563],\n",
       " ['dividend', 5.014044762015715],\n",
       " ['short', 8.917453043894735],\n",
       " ['moon', 0.8413912728308286],\n",
       " ['sell', 14.914631147010471],\n",
       " ['high', 11.484343650084787],\n",
       " ['long', 11.657152473043118],\n",
       " ['💎', 0.17733939135049773],\n",
       " ['🚀', 2.0646447387156486],\n",
       " ['ape', 0.13462260365293258],\n",
       " ['growth', 8.237867785069836],\n",
       " ['dd', 2.349423323366083],\n",
       " ['squeeze', 1.5850517132020765],\n",
       " ['buying', 1.3960622888431518],\n",
       " ['options', 0.016180601400592857]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_stocks if item[0] in bow_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['etf', 7.706877255046216],\n",
       " ['sell', 13.80225250256322],\n",
       " ['long', 13.86486549945605],\n",
       " ['buy', 25.337131855144833],\n",
       " ['dividend', 7.970634504457262],\n",
       " ['etfs', 6.031979588163013],\n",
       " ['growth', 8.526324851881128],\n",
       " ['short', 7.544866125586018],\n",
       " ['high', 12.50772879180396],\n",
       " ['moon', 0.3694166816676972],\n",
       " ['gme', 1.3696593070306569],\n",
       " ['squeeze', 0.6809163412095266],\n",
       " ['cut', 1.8165595723532313],\n",
       " ['buying', 1.0871181585517613],\n",
       " ['dd', 1.1481658305222706],\n",
       " ['💎', 0.11035540702361293],\n",
       " ['🚀', 0.9681534644553843],\n",
       " ['ape', 0.03913312305801877],\n",
       " ['options', 0.02191454891249051]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_investing if item[0] in bow_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['option', 6.096417530797724],\n",
       " ['crypto', 0.20191423982769985],\n",
       " ['dogecoin', 0.29498045841495],\n",
       " ['safe', 0.5948347088257411],\n",
       " ['commodity', 0.22210566381046984],\n",
       " ['risky', 0.24310938645921332],\n",
       " ['longterm', 0.028198367975937392]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_lst2=['option','commodity','longterm','risky','go','safe','crypto','dogecoin']\n",
    "[item for item in bag_of_words_wsb if item[0] in bow_lst2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['safe', 1.6776047532134675],\n",
       " ['longterm', 0.16115878994990487],\n",
       " ['risky', 0.7980272610772398],\n",
       " ['crypto', 0.7054742210658486],\n",
       " ['option', 7.672193960105108],\n",
       " ['commodity', 0.46859021656116917],\n",
       " ['dogecoin', 0.235589556392632]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_stocks if item[0] in bow_lst2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['option', 8.96618115505326],\n",
       " ['crypto', 2.769059787585408],\n",
       " ['safe', 3.1431724440200677],\n",
       " ['longterm', 0.1267913187079808],\n",
       " ['risky', 1.1004234203914878],\n",
       " ['commodity', 0.9877200259843937],\n",
       " ['dogecoin', 0.2371467257315937]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_investing if item[0] in bow_lst2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['index', 0.6424122021184749],\n",
       " ['bullish', 1.036028927115922],\n",
       " ['elon', 1.1400031563605306],\n",
       " ['warren', 0.17301961585235662],\n",
       " ['🦍', 2.8880698855355136],\n",
       " ['fund', 3.3959886371020898],\n",
       " ['musk', 0.5979678608230674],\n",
       " ['sp500', 0.05964593061576881],\n",
       " ['dow', 0.23858372246307524],\n",
       " ['bezos', 0.09353038925352074],\n",
       " ['buffet', 0.13693034654981945],\n",
       " ['bearish', 0.3482440423694984],\n",
       " ['s&amp;p500', 0.07299083727104781]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_lst3=['sp500','dow','musk','elon','bezos','warren','buffet',\n",
    "          'index','fund','s&p500','bullish','bearish','musk','🦍',\n",
    "         's&amp;p500']\n",
    "\n",
    "[item for item in bag_of_words_wsb if item[0] in bow_lst3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['warren', 0.39545389823048943],\n",
       " ['buffet', 0.2815424643703157],\n",
       " ['fund', 5.566774105859967],\n",
       " ['index', 3.4988932468641996],\n",
       " ['bearish', 0.6122739569984337],\n",
       " ['sp500', 0.26406741485767543],\n",
       " ['bullish', 1.603821210826764],\n",
       " ['dow', 0.9320026406741486],\n",
       " ['elon', 0.5727932895809872],\n",
       " ['musk', 0.5928572353177223],\n",
       " ['s&amp;p500', 0.3274953723479994],\n",
       " ['bezos', 0.12815036309269542],\n",
       " ['🦍', 0.03689177119335171]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_stocks if item[0] in bow_lst3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sp500', 0.48290273853595156],\n",
       " ['fund', 21.149105025475663],\n",
       " ['warren', 0.8045770100728659],\n",
       " ['bullish', 1.0088519124357238],\n",
       " ['index', 8.646072208438667],\n",
       " ['bezos', 0.13305261839726382],\n",
       " ['dow', 0.8241435716018753],\n",
       " ['bearish', 0.46490150192926294],\n",
       " ['musk', 0.5642996344966307],\n",
       " ['s&amp;p500', 0.8014463602282245],\n",
       " ['buffet', 0.5228185240551307],\n",
       " ['elon', 0.5048172874484421],\n",
       " ['🦍', 0.018783899067849007]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in bag_of_words_investing if item[0] in bow_lst3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
